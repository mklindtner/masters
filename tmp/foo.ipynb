{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is perhaps the most common technique in applied statistics and machine learning for modelling the relationship between set of a covariates $\\left\\lbrace \\mathbf{x}_n \\right\\rbrace_{n=1}^N$ and a response variable $\\left\\lbrace y_n \\right\\rbrace_{n=1}^N$. Rather than modelling the relationship between $\\mathbf{x}_n$ and $y_n$ directly, we often apply a transformation $\\phi$ to the input vectors $\\mathbf{x}_n$ first. That is, we often use $\\mathbf{\\phi}_n = \\mathbf{\\phi}(\\mathbf{x}_n)$ as input to regression model rather than $\\mathbf{x}_n$ directly:\n",
    "\n",
    "\\begin{align*}\n",
    "y_n = f(\\mathbf{x}_n| \\mathbf{w}) + e_n = \\phi(\\mathbf{x}_n)^T \\mathbf{w} + e_n = \\mathbf{\\phi}_n^T \\mathbf{w} + e_n,\n",
    "\\end{align*}\n",
    "\n",
    "where $f(\\mathbf{x_n}| \\mathbf{w}) \\equiv \\mathbf{\\phi}_n^T \\mathbf{w}$.\n",
    "\n",
    "\n",
    "More generally, let $\\mathbf{\\Phi} \\in \\mathbb{R}^{N \\times D}$ be a **design matrix** and let  $\\mathbf{y} \\in \\mathbb{R}^N$ be the response variables, then the linear regression model is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y}= \\mathbf{\\Phi}\\mathbf{w} + \\mathbf{e},\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^D$ is the regression weights and $\\mathbf{e} \\in \\mathbb{R}^N$ is a noise vector. In this exercise, we will only look at additive isotropic Gaussian noise models, i.e. $e_n \\sim \\mathcal{N}(0, \\sigma^2)$, but later we will study more general set-ups. Recall, the **maximum likelihood estimator** of the weights $\\mathbf{w}$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{w}}_{\\text{MLE}} = \\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^T \\mathbf{y}\n",
    "\\end{align*}\n",
    "\n",
    "We will now turn to the Bayesian treatment. Assuming isotropic Gaussian noise and imposing a multivariate Gaussian prior on $\\mathbf{w} \\sim \\mathcal{N}\\left(\\mathbf{m}_0, \\mathbf{S}_0\\right)$ gives rise to the following joint distribution\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{y}, \\mathbf{w}) = p\\left(\\mathbf{y}|\\mathbf{w}\\right)p\\left(\\mathbf{w}\\right) = \\mathcal{N}\\left(\\mathbf{y}\\big|\\mathbf{\\Phi}\\mathbf{w}, \\sigma^2\\mathbf{I}\\right)\\mathcal{N}\\left(\\mathbf{w}\\big|\\mathbf{m}_0, \\mathbf{S}_0\\right),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma^2$ is the noise variance. We will follow Bishop's notation and use the **noise precision** $\\beta = \\frac{1}{\\sigma^2}$ to parametrize the noise variance.\n",
    "\n",
    "In contrast to the model we worked with last week, this model is an example of a **conjugate** model, meaning that the posterior distribution will also be a Gaussian distribution given by:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{w}|\\mathbf{y}) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{S})\n",
    "\\end{align*}\n",
    "\n",
    "where the posterior covariance is given by\n",
    "\\begin{align*}\n",
    "    \\mathbf{S} &= \\left(\\mathbf{S}_0^{-1} + \\beta \\mathbf{\\Phi}^T\\mathbf{\\Phi}\\right)^{-1}\\tag{1}\n",
    "\\end{align*}\n",
    "and the posterior mean\n",
    "\\begin{align*}\n",
    "\\mathbf{m} &= \\beta \\mathbf{S}\\mathbf{\\Phi}^T\\mathbf{y}.  \\tag{2}\n",
    "\\end{align*}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
